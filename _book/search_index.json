[["index.html", "Bayesian Multi-Object Response Regression Chapter 1 About 1.1 Package Description 1.2 Package Requierments 1.3 Package Installation 1.4 Document Structure", " Bayesian Multi-Object Response Regression Rene Gutierrez Marquez 2023-08-17 Chapter 1 About 1.1 Package Description This work documents the package bmrr introduced in “Multi-object Data Integration in the Study of Primary Progressive Aphasia”. It also presents an usage example in the context of the simulations on the paper for easy replication. BMRR stands for Bayesian Multi-Object Response Regression and deals with a scalar variable of interest as a dependent variable and two objects as responses, one a Network object and another an image object or more generally a Voxel object. Both objects are linked through regions of interest. The main purpose of the model is to determine which regions of interest are associated with the variable of interest. The model can incorporate other explanatory variables. 1.2 Package Requierments The package requires the mvtnorm (Genz et al. 2021; Genz and Bretz 2009) package. 1.3 Package Installation To install the package enter: library(devtools) install_github(&quot;Rene-Gutierrez/bmrr&quot;) 1.4 Document Structure In section 2 we introduce the model and variables References "],["modelFramework.html", "Chapter 2 Model Framework", " Chapter 2 Model Framework The main function of the package is bmrr_sampler which obtains samples according to the method described on the paper in section 3. For every observation \\(i \\in \\{1,\\ldots,n\\}\\): \\(y_i \\in \\mathbb{R}\\): An scalar variable of interest. \\(\\mathbf{A}_i \\in \\mathbb{R}^{P \\times P}\\) A network object, represented as a symmetric matrix of \\(P\\) by \\(P\\), where we disregard the diagonal entries. \\(\\mathbf{g}_{i,p} \\in \\mathbb{R}^{V_p}\\) for \\(p \\in {1,\\ldots,P}\\) voxel elements referenced by the same \\(P\\) regions as the Network object. Notice that we allow for the voxel elements to vary in size across the \\(P\\) regions. \\(\\mathbf{x}_i \\in \\mathbb{R}^H\\) A vector of other covariates that can be taken into consideration. Then we consider the following linear models: \\[\\begin{equation} a_{i,(p,p)} = \\sum_{h=1}^H \\psi^a_{p,h}\\psi^a_{p&#39;,h} x_{i,h} + \\theta_{p,p&#39;}y_i + e_i^{(p,p&#39;) \\quad p &lt; p&#39;} \\tag{2.1} \\end{equation}\\] \\[\\begin{equation} g_{i,(v,p)} = \\sum_{h=1}^H \\psi^g_{p,h} x_{i,h} + \\beta_{v,p}y_i + w_i^{(p,p&#39;)} \\quad {p\\in{1&lt;\\ldots,P}, \\quad v\\in{1,\\ldots,V_p}} \\tag{2.2} \\end{equation}\\] , where \\[\\begin{equation} e_i^{(p,p&#39;)} \\stackrel{iid}{\\sim} N(0, \\tau^2_{\\theta}) \\tag{2.3} \\end{equation}\\] \\[\\begin{equation} w_i^{(v,p)} \\stackrel{iid}{\\sim} N(0, \\tau^2_{\\beta}) \\tag{2.4} \\end{equation}\\] We can also represent this regression equations in stacked format as follows: \\[\\begin{equation} \\mathbf{A}_i = \\sum_{h=1}^H \\mathbf{\\Psi}^a_{p,h} x_{i,h} + \\mathbf{\\Theta}y_i + E_i \\tag{2.5} \\end{equation}\\] \\[\\begin{equation} \\mathbf{g}_{i,p} = \\sum_{h=1}^H \\mathbf{1}_{V_p} \\psi^g_{h,p} x_{i,h} + \\mathbf{\\beta}_p y_i + \\mathbf{w}_{i,p} \\tag{2.6} \\end{equation}\\] where we have stacked the elements as follows: \\[ \\mathbf{A}_i \\in \\mathbb{R}^{P \\times P}, \\quad [\\mathbf{A}_i]_{p,p&#39;}=[\\mathbf{A}_i]_{p&#39;,p}=a_{i,(p,p&#39;)} \\quad \\text{for} \\quad p &lt; p&#39;, \\quad [\\mathbf{A}_i]_{p,p} = 0 \\quad \\text{for} \\quad p = p&#39;\\] \\[ \\mathbf{\\Psi} \\in \\mathbb{R}^{P \\times P}, \\quad [\\mathbf{\\Psi}^a]_{p,p&#39;} = [\\mathbf{\\Psi}^a]_{p&#39;,p} = \\psi_{p,h}^a \\psi_{p&#39;,h}^a, \\quad \\text{for} \\quad p &lt; p&#39;, \\quad [\\mathbf{\\Psi}^a]_{p,p} = 0 \\quad \\text{for} \\quad p = p&#39; \\] \\[ \\mathbf{\\Theta} \\in \\mathbb{R}^{P \\times P}, \\quad [\\mathbf{\\Theta}]_{p,p&#39;} = [\\mathbf{\\Theta}]_{p&#39;,p} = \\theta_{p,p&#39;} \\quad \\text{for} \\quad p &lt; p&#39;, \\quad [\\mathbf{\\Theta}]_{p,p} = 0 \\quad \\text{for} \\quad p = p&#39; \\] \\[ \\mathbf{E}_i \\in \\mathbb{R}^{P \\times P}, \\quad [\\mathbf{E}_i]_{p,p&#39;}=[\\mathbf{E}_i]_{p&#39;,p}=a_{i,(p,p&#39;)} \\quad \\text{for} \\quad p &lt; p&#39;, \\quad [\\mathbf{E}_i]_{p,p} = 0 \\quad \\text{for} \\quad p = p&#39; \\] \\[ \\mathbf{g}_{i,p} \\in \\mathbb{R}^{V_p}, \\quad \\mathbf{g}_{i,p}=(g_{i,1,p},\\ldots,g_{i,V_p,p}) \\quad \\text{for} \\quad p \\in \\{1,\\ldots,P\\}\\] \\[ \\mathbf{1}_{V_p} \\in \\mathbb{R}^{V_p}, \\quad \\text{A vector of ones.}\\] \\[ \\mathbf{w}_{i,p} \\in \\mathbb{R}^{V_p}, \\quad \\mathbf{w}_{i,p}=(w_{i,1,p},\\ldots,w_{i,V_p,p}) \\quad \\text{for} \\quad p \\in \\{1,\\ldots,P\\}\\] The main focus of analysis are the \\(\\mathbf{A}_i\\), \\(\\mathbf{g}_{i,p}\\) and \\(y_i\\) elements. Objects \\(\\mathbf{A}_i\\) and \\(\\mathbf{g}_{i,p}\\) are related through the regions \\(p\\). This connection is developed further in the prior distributions. The prior for the Network coefficients is set as follows: \\[\\begin{equation} \\theta_{p,p&#39;}| \\lambda_{p,p&#39;},\\tau^2_\\theta,\\sigma_\\theta,\\xi_p,\\xi_{p&#39;} \\stackrel{ind.}{\\sim} \\xi_p \\xi_{p&#39;}N(0, \\tau^2_\\theta \\sigma^2_\\theta \\lambda^2_{p,p&#39;}) + (1-\\xi_p \\xi_{p&#39;}) \\delta_0 \\quad p &lt; p&#39;) \\tag{2.7} \\end{equation}\\] while the prior for the Voxel coefficients is: \\[\\begin{equation} \\mathbf{\\beta}_{v,p}|\\phi^2_{v,p},\\eta_p^2,\\tau^2_\\beta, \\xi_p \\stackrel{ind.}{\\sim} \\xi_p N(0, \\tau^2_\\beta \\eta_p^2 \\phi^2_{v,p}) \\mathbf{\\gamma}_p + (1-\\xi_p ) \\delta_0 \\quad v \\in \\{1,\\ldots,V_p\\} \\quad p \\in \\{1,\\ldots,P\\} \\tag{2.8} \\end{equation}\\] That is, a region specific Spike and Slab prior on the Network and Voxel coefficients is applied, that simultaneously determines which regions are relevant through the spike indicators \\(\\xi_p\\) (George and McCulloch 1997). The spike component of the the coefficients is given a Horseshoe structure (Carvalho, Polson, and Scott 2010). Then the Horseshoe structure for the Network coefficients is set as: \\[\\sigma_\\theta \\sim C^+(0,1) \\] \\[\\lambda_{p,p&#39;} \\stackrel{iid}{\\sim} C^+(0,1) \\quad p \\in \\{1,\\ldots,P\\} \\] The Horseshoe structure for the Voxel Coefficients is given by: \\[\\eta^2_p \\stackrel{iid}{\\sim} C^+(0,1) \\quad p \\in \\{1,\\ldots,P\\} \\] \\[\\phi_{v,p} \\stackrel{iid}{\\sim} C^+(0,1) \\quad v \\in \\{1,\\ldots,V_p\\} \\quad p \\in \\{1,\\ldots,P\\} \\] For the spike indicators, the prior is set as: \\[ \\xi_p \\stackrel{iid}{\\sim} Ber(\\nu) \\] \\[ \\nu \\sim Beta(a_\\nu, b_\\nu) \\] where a Beta prior is assigned to the Bernoulli prior probability to account for multiplicity correction. Inverse-Gamma priors are assigned to the error variances: \\[ \\tau^2_\\theta \\sim IG(a_{\\tau_\\theta}, b_{\\tau_\\theta}) \\] \\[ \\tau^2_\\beta \\sim IG(a_{\\tau_\\beta}, b_{\\tau_\\beta}) \\] Finally, for the other covariate coefficients: \\[ \\psi_{p,h}^a,\\psi^g_{p,h} \\stackrel{iid}{\\sim}N(0,1) \\quad p \\in \\{1,\\ldots,P\\}, \\quad h \\in \\{1,\\ldots,H\\} \\] References "],["posteriorComputation.html", "Chapter 3 Posterior Computation 3.1 Full Conditional for \\( {\\boldsymbol \\psi} ^a_{p,.}\\) 3.2 Full conditional for \\( {\\boldsymbol \\psi} ^g_{p,.}\\) 3.3 Full conditional for \\(\\xi_p\\), \\( {\\boldsymbol \\Theta} _{-p,p}\\) and \\( {\\boldsymbol \\beta} _p\\) 3.4 Full Conditional for \\(\\tau^2_\\theta\\) 3.5 Full Conditional for \\(\\tau^2_\\beta\\) 3.6 Full conditional \\(\\nu\\) 3.7 Full Conditional for the Horseshoe Structure", " Chapter 3 Posterior Computation Bayesian estimation of the model is performed through Gibbs sampling which cycles through the following steps: For each \\(p\\in\\{1,...,P\\}\\) sample from \\( {\\boldsymbol \\psi} ^a_{p,.}\\), from the full conditional \\(p( {\\boldsymbol \\psi} ^a_{p,.}| {\\boldsymbol \\psi} ^a_{-p,.}, {\\boldsymbol \\Theta} ,\\tau_\\theta^2, {\\boldsymbol A} )\\). For each \\(p\\in\\{1,...,P\\}\\) sample from \\( {\\boldsymbol \\psi} ^g_{p,.}\\), from the full conditional \\(p( {\\boldsymbol \\psi} ^g_{p,.}| {\\boldsymbol \\beta} _p,\\tau^2_\\beta, {\\boldsymbol g} _p)\\). For each \\(p\\in\\{1,...,P\\}\\) sample jointly \\(\\xi_p\\), \\( {\\boldsymbol \\Theta} _{-p,p}\\) and \\( {\\boldsymbol \\beta} _p\\) from: \\[\\begin{align*} p(\\xi_p, {\\boldsymbol \\Theta} _{-p,p}, {\\boldsymbol \\beta} _p &amp;|\\nu, \\tau_\\theta^2, \\tau_\\beta^2, \\sigma_{\\theta}^2, \\eta_p^2, {\\boldsymbol \\Lambda} _{-p,p}, \\phi_p^2, {\\boldsymbol A} , {\\boldsymbol G} ) \\\\ &amp; = p( {\\boldsymbol \\Theta} _{-p,p}, {\\boldsymbol \\beta} _p | {\\boldsymbol \\xi} , \\tau_\\theta^2, \\tau_\\beta^2, \\sigma_{\\theta}^2, \\eta_p^2, {\\boldsymbol \\Lambda} _{-p,p}, \\phi_p^2, {\\boldsymbol A} , {\\boldsymbol G} ) \\\\ &amp; \\quad \\times p(\\xi_p|\\nu, {\\boldsymbol \\xi} _{-p}, \\tau_\\theta^2, \\tau_\\beta^2, \\sigma_{\\theta}^2, \\eta_p^2, {\\boldsymbol \\Lambda} _{-p,p}, \\phi_p^2, {\\boldsymbol A} , {\\boldsymbol G} ) \\end{align*}\\] Sample \\(\\tau^2_\\theta\\) from the full conditional \\(p(\\tau^2_\\theta| {\\boldsymbol \\xi} , {\\boldsymbol \\Theta} , \\sigma_{\\theta}^2, {\\boldsymbol \\Lambda} , {\\boldsymbol A} )\\) Sample \\(\\tau^2_\\beta\\) from the full conditional: \\(p(\\tau^2_\\beta| {\\boldsymbol \\xi} , {\\boldsymbol \\beta} _1,..., {\\boldsymbol \\beta} _P, \\eta_1^2, ...,\\eta_P^2, \\phi_1^2, ...,\\phi_P^2, {\\boldsymbol G} )\\) Sample \\(\\nu\\) from the full conditional \\(p(\\nu| {\\boldsymbol \\xi} )\\). Sample the horseshoe parameters using the latent variable approach as in (Makalic and Schmidt 2016). 3.1 Full Conditional for \\( {\\boldsymbol \\psi} ^a_{p,.}\\) \\[ {\\boldsymbol \\psi} ^a_{p,.}| {\\boldsymbol \\psi} ^a_{-p,.}, {\\boldsymbol \\Theta} ,\\sigma^2, {\\boldsymbol A} \\sim N \\left( \\hat{ {\\boldsymbol \\psi} }^a_{p,.}, \\tau_\\theta^2 ( {\\boldsymbol W} &#39; {\\boldsymbol W} )^{-1} \\right)\\] where \\[\\begin{align*} \\hat{ {\\boldsymbol \\psi} }^a_{p,.} &amp;= ( {\\boldsymbol W} &#39; {\\boldsymbol W} )^{-1} {\\boldsymbol W} &#39; {\\boldsymbol R} \\\\ {\\boldsymbol R} &amp;= [ ( {\\boldsymbol R} _1)_{-p,p}&#39;,...,( {\\boldsymbol R} _n)_{-p,p}&#39; ]&#39; \\\\ {\\boldsymbol W} &amp;= [ {\\boldsymbol W} _1&#39;,..., {\\boldsymbol W} _n&#39;]&#39; \\\\ ( {\\boldsymbol R} _i)_{-p,p} &amp;= ( {\\boldsymbol A} _i)_{-p,p} - {\\boldsymbol \\Theta} _{-p,p}y_i \\quad \\forall i \\in \\{1,...,n\\} \\\\ {\\boldsymbol W} _i &amp;= {\\boldsymbol \\psi} ^a_{-p,.} \\cdot ({\\boldsymbol 1}_P \\otimes {\\boldsymbol x} _i&#39;) \\quad \\forall i \\in \\{1,...,n\\}. \\\\ \\end{align*}\\] 3.2 Full conditional for \\( {\\boldsymbol \\psi} ^g_{p,.}\\) \\[ {\\boldsymbol \\psi} ^g_{p,.}| {\\boldsymbol \\beta} _p,\\sigma^2, {\\boldsymbol g} _p \\sim N \\left( \\hat{ {\\boldsymbol \\psi} }^g_{p,.}, \\tau_\\beta^2 ( {\\boldsymbol W} &#39; {\\boldsymbol W} )^{-1} \\right)\\] where \\[\\begin{align*} \\hat{ {\\boldsymbol \\psi} }^g_{p,.} &amp;= ( {\\boldsymbol W} &#39; {\\boldsymbol W} )^{-1} {\\boldsymbol W} &#39; {\\boldsymbol R} \\\\ {\\boldsymbol R} &amp;= [ ( {\\boldsymbol R} _1)_{-p,p}&#39;,...,( {\\boldsymbol R} _n)_{-p,p}&#39; ]&#39; \\\\ {\\boldsymbol W} &amp;= [ {\\boldsymbol W} _1&#39;,..., {\\boldsymbol W} _n&#39;]&#39; \\\\ ( {\\boldsymbol R} _i)_{-p,p} &amp;= {\\boldsymbol g} _{i,p} - {\\boldsymbol \\beta} _p y_i \\quad \\forall i \\in \\{1,...,n\\} \\\\ {\\boldsymbol W} _i &amp;= {\\boldsymbol 1}_{V_p} \\otimes {\\boldsymbol x} _i&#39; \\quad \\forall i \\in \\{1,...,n\\}. \\\\ \\end{align*}\\] 3.3 Full conditional for \\(\\xi_p\\), \\( {\\boldsymbol \\Theta} _{-p,p}\\) and \\( {\\boldsymbol \\beta} _p\\) To sample \\( {\\boldsymbol \\Theta} _{-p,p}, {\\boldsymbol \\beta} _p\\) set them to \\( {\\boldsymbol 0} \\) if \\(\\xi_p = 0\\), if \\(\\xi=1\\) then there are 2 cases. If \\(\\xi_{p&#39;} = 0 \\quad \\forall p&#39;\\) then set \\( {\\boldsymbol \\Theta} _{-p,p} = {\\boldsymbol 0} \\) and sample \\( {\\boldsymbol \\beta} _p\\) from: \\[ {\\boldsymbol \\beta} _p |\\tau_\\beta^2, \\eta_p^2, {\\boldsymbol \\phi} _p^2, {\\boldsymbol G} \\sim N(\\hat{b}, \\tau_\\beta^2 \\text{diag}(1/(S_{yy}^2 + 1 / {\\boldsymbol L} )) )\\] where \\[\\begin{align*} \\hat{b} &amp;= {\\boldsymbol S} _{xy} / (S_{yy}^2 + 1 / {\\boldsymbol L} ) \\\\ {\\boldsymbol S} _{xy} &amp;= {\\boldsymbol S} _{gy,p} \\\\ S_{yy}^2 &amp;= {\\boldsymbol y} &#39; {\\boldsymbol y} \\\\ {\\boldsymbol L} &amp;= \\eta_p^2 ( {\\boldsymbol \\phi} _p^2) \\\\ ( {\\boldsymbol S} _{gy,p})_{v} &amp;= \\sum_{i=1}^n R_{i,v,p}^g y_i \\\\ R_{i,v,p}^g &amp;= {\\boldsymbol g} _{i,(v,p)} - \\sum_{h=1}^H \\psi^g_{p,h} x_{i,h} \\\\ \\end{align*}\\] And sample \\(\\xi_p\\) from \\[ \\xi_p|\\nu, \\tau_\\beta^2, \\eta_p^2, {\\boldsymbol \\phi} _p^2, {\\boldsymbol G} \\sim Bernoulli(\\hat{v}_p)\\] where \\[\\begin{align*} \\hat{v}_p &amp;= o_p / (1 + o_p) \\\\ o_p &amp;= \\exp(\\sum_{i=1} c_i) \\frac{\\nu}{1-\\nu}\\\\ c_i &amp;= -\\frac{1}{2} \\left(\\log( {\\boldsymbol L} _i) + \\log\\left(S_{yy}^2 +\\frac{1}{ {\\boldsymbol L} _i}\\right) \\right) + \\hat{b}^2_i \\frac{S_{yy}^2 +\\frac{1}{ {\\boldsymbol L} _i}}{2\\tau_\\beta^2}\\\\ \\end{align*}\\] where \\( {\\boldsymbol L} _i\\) and \\(\\hat{b}\\) are as in the sampling of \\( {\\boldsymbol \\beta} _p\\). If \\(\\exists p&#39; \\ni \\xi_p \\neq0\\), then sample \\( {\\boldsymbol \\Theta} _{-p,p}\\) and \\( {\\boldsymbol \\beta} _p\\) from: \\[ {\\boldsymbol \\Theta} _{-p,p}, {\\boldsymbol \\beta} _p | {\\boldsymbol \\xi} _{-p}, \\tau_\\beta^2, \\tau_\\theta^2 \\sigma_{\\theta}^2, \\eta_p^2, {\\boldsymbol \\Lambda} _{-p,p}, \\phi_p^2, {\\boldsymbol A} , {\\boldsymbol G} \\sim N(\\hat{b}, \\text{diag}(\\tau/(S_{yy}^2 + 1 / {\\boldsymbol L} ) ) )\\] where \\[\\begin{align*} \\hat{b} &amp;= {\\boldsymbol S} _{xy} / (S_{yy}^2 + 1 / {\\boldsymbol L} ) \\\\ {\\boldsymbol S} _{xy} &amp;= [ ( {\\boldsymbol S} _{ay})_{p, -p}[ {\\boldsymbol \\xi} _{-p} = 1]&#39;, {\\boldsymbol S} _{gy,p}&#39;]&#39; \\\\ S_{yy}^2 &amp;= {\\boldsymbol y} &#39; {\\boldsymbol y} \\\\ \\tau &amp;= (\\tau_\\theta^2 {\\boldsymbol 1} _P[ {\\boldsymbol \\xi} _{-p} = 1]&#39;, \\tau_\\beta^2 {\\boldsymbol 1} _{V_p}&#39;)&#39; \\\\ {\\boldsymbol L} &amp;= [\\sigma^2_{\\theta} {\\boldsymbol \\Lambda} _{p, -p}[ {\\boldsymbol \\xi} _{-p} = 1]&#39;,\\eta_p^2 ( {\\boldsymbol \\phi} _p^2)&#39;]&#39; \\\\ ( {\\boldsymbol S} _{ay})_{p,p&#39;} &amp;= \\sum_{i=1}^n (R_i^a)_{p,p&#39;}y_i \\\\ ( {\\boldsymbol S} _{gy,p})_{v} &amp;= \\sum_{i=1}^n R_{i,v,p}^g y_i \\\\ (R_i^a)_{p,p&#39;} &amp;= ( {\\boldsymbol A} _i)_{p,p&#39;} - \\sum_{h=1}^H (\\psi^a)_{p,h} (\\psi^a)_{p&#39;,h} x_{i,h} \\\\ R_{i,v,p}^g &amp;= {\\boldsymbol g} _{i,(v,p)} - \\sum_{h=1}^H \\psi^g_{p,h} x_{i,h} \\\\ \\end{align*}\\] We sample \\(\\xi_p\\) from \\[ \\xi_p|\\nu, {\\boldsymbol \\xi} _{-p}, \\tau_\\theta^2, \\tau_\\beta^2, \\sigma_{\\theta}^2, \\eta_p^2, {\\boldsymbol \\Lambda} _{-p,p}, \\phi_p^2, {\\boldsymbol A} , {\\boldsymbol G} \\sim Bernoulli(\\hat{v}_p)\\] where \\[\\begin{align*} \\hat{v}_p &amp;= o_p / (1 + o_p) \\\\ o_p &amp;= \\exp(\\sum_{i=1} c_i) \\frac{\\nu}{1-\\nu}\\\\ c_i &amp;= -\\frac{1}{2} \\left(\\log( {\\boldsymbol L} _i) + \\log\\left(S_{yy}^2 +\\frac{1}{ {\\boldsymbol L} _i}\\right) \\right) + \\hat{b}_i \\frac{S_{yy}^2 +\\frac{1}{ {\\boldsymbol L} _i}}{2\\tau^2}\\\\ \\end{align*}\\] where \\( {\\boldsymbol L} _i\\) and \\(\\hat{b}\\) and \\(\\tau\\) are as in the sampling of \\( {\\boldsymbol \\Theta} _{-p,p}, {\\boldsymbol \\beta} _p\\). 3.4 Full Conditional for \\(\\tau^2_\\theta\\) \\[ \\tau^2_\\theta| {\\boldsymbol \\xi} , {\\boldsymbol \\Theta} , \\sigma_{\\theta}^2, {\\boldsymbol \\Lambda} , {\\boldsymbol A} \\sim IG(\\hat{a}_{\\tau_\\theta}, \\hat{b}_{\\tau\\theta}) \\] where \\[\\begin{align*} \\hat{b}_{\\tau_\\theta} &amp; = b_{\\tau_\\theta} + \\frac{\\sum_{i=1}^n \\sum_{p&lt;p&#39;} (R^a_i)_{p,p&#39;}}{2} \\\\ &amp;+ \\frac{\\sum_{p&lt;p&#39;} {\\boldsymbol \\Theta} _{ {\\boldsymbol \\xi} = 1, {\\boldsymbol \\xi} = 1}^2 / {\\boldsymbol \\Lambda} _{ {\\boldsymbol \\xi} = 1, {\\boldsymbol \\xi} = 1} / \\sigma^2_{\\theta}}{2} \\\\ \\hat{a}_{\\tau_\\theta} &amp; = a_{\\tau_\\theta} + \\frac{\\frac{np(p-1)}{2} + \\frac{nq(q-1)}{2}}{2} \\\\ R_i^a &amp;= \\left( {\\boldsymbol A} _i - \\sum_{h=1}^{H} {\\boldsymbol \\psi} _{h}^a ( {\\boldsymbol \\psi} _{h}^{a})&#39; x_{i,h} - {\\boldsymbol \\Theta} \\: y_i \\right)^2 \\\\ \\end{align*}\\] 3.5 Full Conditional for \\(\\tau^2_\\beta\\) \\[ \\tau^2_\\beta| {\\boldsymbol \\xi} , {\\boldsymbol \\beta} _1,..., {\\boldsymbol \\beta} _P, \\eta_1^2, ...,\\eta_P^2, \\phi_1^2, ...,\\phi_P^2, {\\boldsymbol G} \\sim IG(\\hat{a}_{\\tau_\\beta}, \\hat{b}_{\\tau_\\beta}) \\] where \\[\\begin{align*} \\hat{b}_{\\tau_\\beta} &amp;= b_{\\tau_\\beta} + \\frac{\\sum_{i=1}^n \\sum_{p=1}^P R^g_{i,p} + \\sum_{p=1}^P \\xi_p {\\boldsymbol \\beta} _p^2 / (\\eta_p \\phi^2_p)}{2} \\\\ \\hat{a}_{\\tau_\\beta} &amp;= a_{\\tau_\\beta} + \\frac{n \\sum_{p=1}^P V_p + n \\sum_{p=1}^P V_p \\xi_p}{2} \\\\ R_{i,p}^g &amp;= \\left( {\\boldsymbol g} _{i,p} - \\sum_{h=1}^{H} {\\boldsymbol 1}_{V_p}\\psi_{p,h}^g x_{i,h} - {\\boldsymbol \\beta} _{p}\\: y_i \\right)^2 \\end{align*}\\] 3.6 Full conditional \\(\\nu\\) \\[ \\nu|\\xi_1,\\ldots,\\xi_p \\sim Beta \\left(a_\\nu + \\sum_{p=1}^P \\xi_p, b_{\\nu}+P-\\sum_{p=1}^P \\xi_p \\right) \\] 3.7 Full Conditional for the Horseshoe Structure Following (Makalic and Schmidt 2016) auxiliary variables are used for the priors of the \\(\\lambda_{p,p&#39;}\\)’s, \\(\\sigma^2_\\theta\\), \\(\\phi_{v.p}\\)’2 and \\(\\eta_p\\)’s as follows: \\[ \\lambda_{p,p&#39;} \\sim IG(1/2, 1/\\kappa_{\\lambda_{p,p&#39;}}) \\] \\[ \\sigma^2_\\theta \\sim IG(1/2, 1/\\kappa_{\\sigma^2_\\theta}) \\] \\[ \\phi_{v,p} \\sim IG(1/2, 1/\\kappa_{\\phi_{v,p}}) \\] \\[ \\eta_p \\sim IG(1/2, 1/\\kappa_{\\eta_p}) \\] \\[ \\kappa_{\\lambda_{p,p}}, \\kappa_{\\sigma_\\theta}, \\kappa_{\\phi_{v,p}}, \\kappa_{\\eta_p} \\stackrel{iid}{\\sim} IG(1/2, 1) \\quad v \\in \\{1,\\ldots,V_p\\} \\quad p \\in \\{1,\\ldots,P\\} \\] Then the full conditionals for the Network Horseshoe parameters (including auxiliary variables) are given by \\[ \\lambda^2_{p,p&#39;}| \\sigma^2_\\theta, \\tau^2_\\theta, \\theta_{p,p&#39;}, \\xi_p, \\xi_{p&#39;} \\stackrel{ind.}{\\sim} IG \\left(1, \\frac{1}{\\kappa_{\\lambda_{p,p&#39;}}}+I_{\\xi_p=1}I_{\\xi_{p&#39;}=1}\\frac{\\theta^2_{p,p&#39;}}{2 \\sigma^2_\\theta \\tau^2_\\theta}\\right) \\quad p&lt;p&#39;\\] \\[ \\sigma^2_\\theta| {\\boldsymbol \\Lambda} , \\tau^2_\\theta, {\\boldsymbol \\Theta} , \\xi_p \\stackrel{ind.}{\\sim} IG \\left(1, \\frac{1}{\\kappa_{\\sigma^2_\\theta}} + \\frac{1}{2 \\tau^2_\\theta} \\sum_{\\substack{p&lt;p&#39; \\\\ \\xi_p = 1 \\\\ \\xi_{p&#39;}=1}}\\frac{\\theta_{p,p&#39;}^2}{\\tau^2_{\\theta}\\lambda^2_{p,p&#39;}} \\right) \\] \\[ \\kappa_{\\lambda_{p,p&#39;}}|\\lambda^2_{p,p&#39;} \\sim IG \\left(1, 1 + \\frac{1}{\\lambda^2_{p,p&#39;}}\\right) \\] \\[ \\kappa_{\\sigma_\\theta}|\\sigma^2_\\theta \\sim IG \\left(1, 1 + \\frac{1}{\\sigma^2_\\theta}\\right) \\] Then the full conditionals for the Voxel Horseshoe parameters (including auxiliary variables) are given by \\[ \\phi^2_{v,p}| \\eta^2_p, \\tau^2_\\beta, \\phi_{v,p}, \\xi_p \\stackrel{ind.}{\\sim} IG \\left(1, \\frac{1}{\\kappa_{\\phi_{v,p}}}+I_{\\xi_p=1}\\frac{ \\beta^2_{v,p}}{2 \\eta^2_p \\tau^2_\\beta}\\right) \\quad v \\in \\{1,\\ldots,V_p\\} \\quad p \\in \\{1,\\ldots,P\\} \\] \\[ \\eta^2_p| {\\boldsymbol \\Phi} , \\tau^2_\\beta, {\\boldsymbol B} , \\xi_p \\stackrel{ind.}{\\sim} IG \\left(1, \\frac{1}{\\kappa_{\\eta_P}} + \\frac{I_{\\xi_p = 1}}{2 \\tau^2_\\beta} \\sum_{\\substack{v=1}}^{V_p}\\frac{\\beta_{v,p}^2}{\\tau^2_{\\theta}\\lambda^2_{p,p&#39;}} \\right) \\quad p \\in \\{1,\\ldots,P\\} \\] \\[ \\kappa_{\\phi_{v,p}}|\\phi^2_{v,p} \\sim IG \\left(1, 1 + \\frac{1}{\\phi^2_{v,p}}\\right) \\] \\[ \\kappa_{\\eta_p}|\\eta^2_p \\sim IG \\left(1, 1 + \\frac{1}{\\eta^2_p}\\right) \\quad p \\in \\{1,\\ldots,P\\} \\] References "],["functions.html", "Chapter 4 Functions 4.1 bmrr_sampler", " Chapter 4 Functions In this section the package functions are explained (including those that are not available to use to the user). In this section variable names in the code are written in code mode while the variables described in Model Framework are written in math mode, for example in the code the main variable is y while in model the main variable is written \\(y\\). Unfortunately, the name of the variables in the code and the model framework are not always a close match as in the case of y and \\(y\\). The corresponding relations will be indicated when each variable is introduced in the code. 4.1 bmrr_sampler 4.1.1 Desription The main function of the package is bmrr_sampler. It is in charge of receiving the data and model set-up and returns samples for all the model parameters, as well as the last state of the variables. The first part of the function deals with the set-up, while the second part loops through the function bmrr_iterator to obtain and save the samples. The actual computation of the full conditionals is done by bmrr_iterator. 4.1.2 Inputs bmrr_sampler receives three types of inputs, data, hyper-parameters and set-up inputs. The data inputs required by the function. They are as follows: Code Model Description y \\( {\\boldsymbol y} \\) This is the variable of interest. It should be inputted as a vector of size \\(n\\), the number of observations. A \\( {\\boldsymbol A} \\) The Network object. It should be inputted as an array of size \\((n \\times P \\times P)\\). For each observation \\(i\\), A[i,,] must be symmetric with NA as diagonal elements. G \\(\\{ {\\boldsymbol g} _p\\}_{p=1}^P\\) The voxel object. It should be inputted as an array of size \\((n \\times V \\times P)\\), where \\(V= \\max(\\{V_p\\}_{p=1}^P)\\). If the regions are not the same size (i.e. there is a \\(p\\) for which \\(V_p &lt; V\\)), then pad the entry G[i,p,] with NA’s, do not use 0 or any other numeric value. X \\( {\\boldsymbol X} \\) Other covariates to take in consideration. It should be inputted as a matrix of size \\((n \\times H)\\). Each column X[,h] represents one covariate. The hyper-parameter inputs are not required, if they are not specified the default values are used. They are as follows: Code Model Description a_sT \\(a_{\\tau_\\theta}\\) Shape hyper-parameter of the Inverse-gamma prior for \\(\\tau_\\theta^2\\). An scalar. By default, a_sT = 1. b_sT \\(b_{\\tau_\\theta}\\) Scale hyper-parameter of the Inverse-gamma prior for \\(\\tau_\\theta^2\\). An scalar. By default, b_sT = 1. a_sB \\(a_{\\tau_\\beta}\\) Shape hyper-parameter of the Inverse-gamma prior for \\(\\tau_\\beta^2\\). An scalar. By default, a_sB = 1. b_sB \\(b_{\\tau_\\beta}\\) Scale hyper-parameter of the Inverse-gamma prior for \\(\\tau_\\beta^2\\). An scalar. By default, b_sB = 1. a_nu \\(a_{\\nu}\\) Shape 1 hyper-parameter of the Beta prior for \\(\\nu\\). An scalar. By default, a_sB = 1. b_nu \\(b_{\\nu}\\) Shape 2 hyper-parameter of the Beta prior for \\(\\nu\\). An scalar. By default, b_sB = 1. The set-up inputs also have set defaults, however, unlike the hyper-parameter inputs, the set-up inputs are might require more attention from the user as they deal with the characteristics of the MCMC samples. As set-up inputs, they do not have a model equivalent. They are as follows: Code Description nmcmc Number of MCMC samples of the output. Default is 1000. Notice this number, does not include the samples that are lost due to burn-in and thinning. burnin Number of initial iterations of the MCMC to discard. Default is 0, that is there is no burn-in. thinning Number of iterations of the MCMC chain to obtain one output sample. state A list of containing all the model parameters. It can be used in two ways. The first one is to initialize the MCMC at a selected starting point. The second one is to continue a MCMC sample chain. It is also an output of the function, so it is readily available in case the function is used sequentially. small_out A Boolean that indicates if a small set of the parameters of the model is required for memory purposes. By default small_out = FALSE, which returns all the model parameters. If small_out = TRUE, then only the first order parameters in the hierarchy and the indicators are returned, that is the parameters that appear on equations (2.1), (2.2), (2.3), (2.4) and \\( {\\boldsymbol \\xi} \\). Further more the network and voxel coeeficients are returned in a single object. 4.1.3 Output The function returns a list with two elements. The first one, and most important is a list called sam that contains nmcmc samples of the model parameters after discarding burn-in iterations according to burnin and skipping iterations according to thinning. For example, if: nmcmc = 100 burnin = 10 thinning = 3 the function will perform 10 + 100 * 3 = 310 MCMC iterations to return 100 samples for each parameter, as specified by small_out. The second output of the function is state a list all the variables on the last iteration of the MCMC. If small_out = FALSE (by default) samples for every model parameter and auxiliary variables are returned. Code Model Description g \\( {\\boldsymbol \\xi} \\) Samples for the region indicator variables. It is returned as a binary matrix of size nmcmc rows by \\(P\\). For sample \\(s\\), g[s,p] \\(={\\xi_p^s}&#39;\\). nu \\(\\nu\\) Samples of the probability parameter of the Bernoulli prior of the indicators \\(\\xi_p\\). It is as vector of size nmcmc. For sample \\(s\\), nu[s]\\(=\\nu^s\\). Theta \\( {\\boldsymbol \\Theta} \\) Samples for the Network coefficients. It is an array of size (nmcmc\\(\\times P \\times P\\)). For sample \\(s\\), Theta[s,p,q]\\(=( {\\boldsymbol \\Theta} ^s)_{p,q}=\\theta_{p,q}^s\\) for \\(p\\neq q\\) and Theta[s,p,p]=NA. B \\(\\{ {\\boldsymbol g} _p\\}_{p=1}^P\\) Samples for the Voxel coefficients. It is an array of size (nmcmc\\(\\times V \\times P\\)), where \\(V=\\max\\{V_p\\}_{p=1}^P\\). For sample \\(s\\), B[s,v,p]\\(=\\beta_{v,p}\\) for \\(v \\leq V_p\\), and B[s,v,p]=NA for \\(v &gt; V_p\\). DA \\(\\{ {\\boldsymbol \\psi} _{p,h}^a\\}_{p=1,h=1}^{P,H}\\) Samples of the bilinear structure of the Network equation coefficients of the covariates \\(X\\). It is an array of size (nmcmc \\(\\times P \\times H\\)). For sample \\(s\\), DA[s,p,h]\\(= {\\psi_{p,h}^{a,s}}&#39;\\). DG \\(\\{ {\\boldsymbol \\psi} _{p,h}^g\\}_{p=1,h=1}^{P,H}\\) Samples of the structure of the Voxel equation coefficients of the covariates \\(X\\). It is an array of size (nmcmc \\(\\times P \\times H\\)). For sample \\(s\\), DA[s,p,h]\\(= {\\psi_{p,h}^{g,s}}&#39;\\). sT2 \\(\\tau^2_\\theta\\) Samples of the variance of the error of the Network equation. It is a vector of size nmcmc. For sample \\(s\\), sT2[s] \\(= {\\tau^2_\\theta}^s\\). sB2 \\(\\tau^2_\\beta\\) Samples of the variance of the error of the Voxel equation. It is a vector of size nmcmc. For sample \\(s\\), sB2[s] \\(= {\\tau^2_\\beta}^s\\). t2T \\(\\sigma_\\theta^2\\) Samples of the global shrinking parameter for the Horseshoe structure of the Network coefficients. It is a vector of size nmcmc. For sample \\(s\\), t2T[s]\\(={\\sigma^2_\\theta}^s\\). l2T \\( {\\boldsymbol \\Lambda} \\) Samples of the local shrinking parameter for the Horseshoe structure of the Network coefficients. It is an array of size nmcmc\\(\\times P \\times P\\). For sample \\(s\\), l2T[s,p,q]\\(=( {\\boldsymbol \\Lambda} )_{p,q}^s = \\lambda_{p,q}^s\\) for \\(p \\neq q\\) and l2T[s,p,p]=NA. xiT \\(\\kappa_{\\sigma_\\theta^2}\\) Samples of the auxiliary variable of the global shrinking parameter for the Horseshoe structure of the Network coefficients. It is a vector of size nmcmc. For sample \\(s\\), xiT[s]\\(=\\kappa^s_{\\sigma^2_\\theta}\\). vT \\(\\kappa_{\\lambda_{p,p&#39;}}\\) Samples of the auxiliary variables of the local shrinking parameter for the Horseshoe structure of the Network coefficients. It is an array of size nmcmc\\(\\times P \\times P\\). For sample \\(s\\), vT[s,p,q]\\(=( {\\boldsymbol \\Lambda} )_{p,q}^s = \\kappa_{\\lambda_{p,q}}^s\\) for \\(p \\neq q\\) and l2T[s,p,p] = NA. t2B \\(\\{\\eta_p\\}_{p=1}^P\\) Samples of the global shrinking parameters for the Horseshoe structure of the Voxel coefficients. It is a matrix of size nmcmc\\(\\times P\\). For sample \\(s\\), t2B[s,p]\\(={\\eta_p}^s\\). l2B \\(\\{\\phi_{v,p}\\}_{v=1,p=1}^{V_p,P}\\) Samples of the local shrinking parameter for the Horseshoe structure of the Network coefficients. It is an array of size nmcmc\\(\\times V \\times P\\). For sample \\(s\\), l2B[s,v,p]\\(=\\phi_{v,p}^s = \\phi_{v,p}^s\\) for \\(v \\leq V_p\\) and l2B[s,v,p]=NA for \\(v&gt;V_p\\). xiB \\(\\kappa_{\\eta_p}\\) Samples of the auxiliary variables of the global shrinking parameter for the Horseshoe structure of the Voxel coefficients. It is a matrix of size nmcmc\\(\\times P\\). For sample \\(s\\), xiT[s,p]\\(=\\kappa^s_{\\eta_p}\\). vB \\(\\kappa_{\\phi_{v,p}}\\) Samples of the auxiliary variables of the local shrinking parameter for the Horseshoe structure of the Voxel coefficients. It is an array of size nmcmc\\(\\times V \\times P\\). For sample \\(s\\), vB[s,v,p]\\(=\\kappa_{\\phi_{v,p}}^s\\) for \\(v \\leq V_p\\) and vB[s,v,p] = NA for \\(v&gt; V_p\\). If small_out = TRUE only the samples for the following parameters are returned: Code Model Description g \\( {\\boldsymbol \\xi} \\) Samples for the region indicator variables. It is returned as a binary matrix of size nmcmc rows by \\(P\\). For sample \\(s\\), g[s,p] \\(={\\xi_p^s}&#39;\\). B \\( {\\boldsymbol \\Theta} \\) and \\(\\{ {\\boldsymbol g} _p\\}_{p=1}^P\\) Samples for the network and voxel coefficients. A matrix of size (nmcmc \\(\\times P(P - 1) / P + V\\)). The order of the coefficients is obtained by stacking the coefficients of the network object and then the coefficients for the voxel object. The coefficients of the network object are ordered by upper.tri applied to \\( {\\boldsymbol \\Theta} \\) and the coefficients of the voxel object are ordered by stacking the \\(\\{ {\\boldsymbol \\beta} _p\\}_{p = 1}^P\\) objects. That is each row of B is a sample \\(s\\), such that B[s,] = \\((\\theta_{1,2}^s,\\theta_{1,3}^s,\\theta_{2,3}^s,\\ldots,\\theta_{1,P}^s,\\ldots,\\theta_{P-1,P}^s,{ {\\boldsymbol \\beta} _1^s}&#39;,\\ldots,{ {\\boldsymbol \\beta} _P^s}&#39;)\\). DA \\(\\{ {\\boldsymbol \\psi} _{p,h}^a\\}_{p=1,h=1}^{P,H}\\) Samples of the bilinear structure of the Network equation coefficients of the covariates \\(X\\). It is an array of size (nmcmc \\(\\times P \\times H\\)). For sample \\(s\\), DA[s,p,h]\\(= {\\psi_{p,h}^{a,s}}&#39;\\). DG \\(\\{ {\\boldsymbol \\psi} _{p,h}^g\\}_{p=1,h=1}^{P,H}\\) Samples of the structure of the Voxel equation coefficients of the covariates \\(X\\). It is an array of size (nmcmc \\(\\times P \\times H\\)). For sample \\(s\\), DA[s,p,h]\\(= {\\psi_{p,h}^{g,s}}&#39;\\). sT2 \\(\\tau^2_\\theta\\) Samples of the variance of the error of the Network equation. It is a vector of size nmcmc. For sample \\(s\\), sT2[s] \\(= {\\tau^2_\\theta}^s\\). sB2 \\(\\tau^2_\\beta\\) Samples of the variance of the error of the Voxel equation. It is a vector of size nmcmc. For sample \\(s\\), sB2[s] \\(= {\\tau^2_\\beta}^s\\). The state output is a list containing all the model variables (including auxiliaries). 4.1.4 Walkthrough 4.1.4.1 Problem Dimensions Obtains the sample size and dimensions of the objects. N\\(=n\\). The number of observations. mV\\(V=\\max\\{V_p\\}_{p=1}^P\\). The maximum number of voxels per region. P\\(=P\\). The number of regions. H\\(=H\\). Number of other covariates (other than the main variable). 4.1.4.2 Creates Auxiliary Variables This variables are used on every iteration of the MCMC bmrr_iterator. C. Is a Boolean matrix of size \\(V \\times P\\) that indicates which entries of the voxel data are valid entries. Zv It is a transformation of the Network and Voxel data objects into one matrix. Each row is an observation of the Network object followed by the voxel object. Sometimes is convenient to work with the original data objects and sometimes is more convenient to work with the stacked object. XG. A replication of the covariates \\(V\\) times, stacked one over the other it is used for the computation of \\(\\{\\psi_{v,p}^g\\}_{v=1,p=1}^{V_p,P}\\). 4.1.4.3 Sample Holders Creates the holders of the samples according to small_out. 4.1.4.4 Initialization Initializes the MCMC, if state is provided, then it is used as the initial value otherwise default values are used. 4.1.4.5 Progress Bar Creates the progress bar to show the progress of the MCMC. 4.1.4.6 First Run Computes the first sample of the MCMC (it is discarded in every case). 4.1.4.7 Sampling Goes through the MCMC by calling bmrr_iterator, saves the samples after considering thinning and burnin as well as small_out and updates the progress bar. 4.1.4.8 Saves the Samples Saves the samples on a list according to small_out. 4.1.4.9 Saves the State Saves the last iteration of the MCMC in a list. 4.1.4.10 Returns Values Returns the two outputs, the MCMC samples (sam) and the last iteration of the MCMC (state). "],["references.html", "References", " References "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]

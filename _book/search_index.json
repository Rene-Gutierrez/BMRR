[["index.html", "Bayesian Multi-Object Response Regression Chapter 1 About 1.1 Package Description 1.2 Package Requierments 1.3 Package Installation 1.4 Document Structure", " Bayesian Multi-Object Response Regression Rene Gutierrez Marquez 2023-08-17 Chapter 1 About 1.1 Package Description This work documents the package bmrr introduced in “Multi-object Data Integration in the Study of Primary Progressive Aphasia”. It also presents an usage example in the context of the simulations on the paper for easy replication. BMRR stands for Bayesian Multi-Object Response Regression and deals with a scalar variable of interest as a dependent variable and two objects as responses, one a Network object and another an image object or more generally a Voxel object. Both objects are linked through regions of interest. The main purpose of the model is to determine which regions of interest are associated with the variable of interest. The model can incorporate other explanatory variables. 1.2 Package Requierments The package requires the mvtnorm (Genz et al. 2021; Genz and Bretz 2009) package. 1.3 Package Installation To install the package enter: library(devtools) install_github(&quot;Rene-Gutierrez/bmrr&quot;) 1.4 Document Structure In section 2 we introduce the model and variables References "],["modelFramework.html", "Chapter 2 Model Framework", " Chapter 2 Model Framework The main function of the package is bmrr_sampler which obtains samples according to the method described on the paper in section 3. For every observation \\(i \\in \\{1,\\ldots,n\\}\\): \\(y_i \\in \\mathbb{R}\\): An scalar variable of interest. \\(\\mathbf{A}_i \\in \\mathbb{R}^{P \\times P}\\) A network object, represented as a symmetric matrix of \\(P\\) by \\(P\\), where we disregard the diagonal entries. \\(\\mathbf{g}_{i,p} \\in \\mathbb{R}^V_p\\) for \\(p \\in {1,\\ldots,P}\\) voxel elements referenced by the same \\(P\\) regions as the Network object. Notice that we allow for the voxel elements to vary in size across the \\(P\\) regions. \\(\\mathbf{x}_i \\in \\mathbb{R}^H\\) A vector of other covariates that can be taken into consideration. Then we consider the following linear models: \\[\\begin{equation} a_{i,(p,p)} = \\sum_{h=1}^H \\psi^a_{p,h}\\psi^a_{p&#39;,h} x_{i,h} + \\theta_{p,p&#39;}y_i + e_i^{(p,p&#39;) \\quad p &lt; p&#39;} \\tag{2.1} \\end{equation}\\] \\[\\begin{equation} g_{i,(v,p)} = \\sum_{h=1}^H \\psi^g_{p,h} x_{i,h} + \\beta_{v,p}y_i + w_i^{(p,p&#39;)} \\quad {p\\in{1&lt;\\ldots,P}, \\quad v\\in{1,\\ldots,V_p}} \\tag{2.2} \\end{equation}\\] , where \\[e_i^{(p,p&#39;)} \\stackrel{iid}{\\sim} N(0, \\tau^2_{\\theta})\\] \\[w_i^{(v,p)} \\stackrel{iid}{\\sim} N(0, \\tau^2_{\\beta})\\] We can also represent this regression equations in stacked format as follows: \\[\\begin{equation} \\mathbf{A}_i = \\sum_{h=1}^H \\mathbf{\\Psi}^a_{p,h} x_{i,h} + \\mathbf{\\Theta}y_i + E_i \\tag{2.3} \\end{equation}\\] \\[\\begin{equation} \\mathbf{g}_{i,p} = \\sum_{h=1}^H \\mathbf{1}_{V_p} \\psi^g_{h,p} x_{i,h} + \\mathbf{\\beta}_p y_i + \\mathbf{w}_{i,p} \\tag{2.4} \\end{equation}\\] where we have stacked the elements as follows: \\[ \\mathbf{A}_i \\in \\mathbb{R}^{P \\times P}, \\quad [\\mathbf{A}_i]_{p,p&#39;}=[\\mathbf{A}_i]_{p&#39;,p}=a_{i,(p,p&#39;)} \\quad \\text{for} \\quad p &lt; p&#39;, \\quad [\\mathbf{A}_i]_{p,p} = 0 \\quad \\text{for} \\quad p = p&#39;\\] \\[ \\mathbf{\\Psi} \\in \\mathbb{R}^{P \\times P}, \\quad [\\mathbf{\\Psi}^a]_{p,p&#39;} = [\\mathbf{\\Psi}^a]_{p&#39;,p} = \\psi_{p,h}^a \\psi_{p&#39;,h}^a, \\quad \\text{for} \\quad p &lt; p&#39;, \\quad [\\mathbf{\\Psi}^a]_{p,p} = 0 \\quad \\text{for} \\quad p = p&#39; \\] \\[ \\mathbf{\\Theta} \\in \\mathbb{R}^{P \\times P}, \\quad [\\mathbf{\\Theta}]_{p,p&#39;} = [\\mathbf{\\Theta}]_{p&#39;,p} = \\theta_{p,p&#39;} \\quad \\text{for} \\quad p &lt; p&#39;, \\quad [\\mathbf{\\Theta}]_{p,p} = 0 \\quad \\text{for} \\quad p = p&#39; \\] \\[ \\mathbf{E}_i \\in \\mathbb{R}^{P \\times P}, \\quad [\\mathbf{E}_i]_{p,p&#39;}=[\\mathbf{E}_i]_{p&#39;,p}=a_{i,(p,p&#39;)} \\quad \\text{for} \\quad p &lt; p&#39;, \\quad [\\mathbf{E}_i]_{p,p} = 0 \\quad \\text{for} \\quad p = p&#39; \\] \\[ \\mathbf{g}_{i,p} \\in \\mathbb{R}^{V_p}, \\quad \\mathbf{g}_{i,p}=(g_{i,1,p},\\ldots,g_{i,V_p,p}) \\quad \\text{for} \\quad p \\in \\{1,\\ldots,P\\}\\] \\[ \\mathbf{1}_{V_p} \\in \\mathbb{R}^{V_p}, \\quad \\text{A vector of ones.}\\] \\[ \\mathbf{w}_{i,p} \\in \\mathbb{R}^{V_p}, \\quad \\mathbf{w}_{i,p}=(w_{i,1,p},\\ldots,w_{i,V_p,p}) \\quad \\text{for} \\quad p \\in \\{1,\\ldots,P\\}\\] The main focus of analysis are the \\(\\mathbf{A}_i\\), \\(\\mathbf{g}_{i,p}\\) and \\(y_i\\) elements. Objects \\(\\mathbf{A}_i\\) and \\(\\mathbf{g}_{i,p}\\) are related through the regions \\(p\\). This connection is developed further in the prior distributions. The prior for the Network coefficients is set as follows: \\[\\begin{equation} \\theta_{p,p&#39;}| \\lambda_{p,p&#39;},\\tau^2_\\theta,\\sigma_\\theta,\\xi_p,\\xi_{p&#39;} \\stackrel{ind.}{\\sim} \\xi_p \\xi_{p&#39;}N(0, \\tau^2_\\theta \\sigma^2_\\theta \\lambda^2_{p,p&#39;}) + (1-\\xi_p \\xi_{p&#39;}) \\delta_0 \\quad p &lt; p&#39;) \\tag{2.5} \\end{equation}\\] while the prior for the Voxel coefficients is: \\[\\begin{equation} \\mathbf{\\beta}_{v,p}|\\phi^2_{v,p},\\eta_p^2,\\tau^2_\\beta, \\xi_p \\stackrel{ind.}{\\sim} \\xi_p N(0, \\tau^2_\\beta \\eta_p^2 \\phi^2_{v,p}) \\mathbf{\\gamma}_p + (1-\\xi_p ) \\delta_0 \\quad v \\in \\{1,\\ldots,V_p\\} \\quad p \\in \\{1,\\ldots,P\\} \\tag{2.6} \\end{equation}\\] That is, a region specific Spike and Slab prior on the Network and Voxel coefficients is applied, that simultaneously determines which regions are relevant through the spike indicators \\(\\xi_p\\) (George and McCulloch 1997). The spike component of the the coefficients is given a Horseshoe structure (Carvalho, Polson, and Scott 2010). Then the Horseshoe structure for the Network coefficients is set as: \\[\\sigma_\\theta \\sim C^+(0,1) \\] \\[\\lambda_{p,p&#39;} \\stackrel{iid}{\\sim} C^+(0,1) \\quad p \\in \\{1,\\ldots,P\\} \\] The HOrseshoe structure for the Voxel Coefficients is given by: \\[\\eta^2_p \\stackrel{iid}{\\sim} C^+(0,1) \\quad p \\in \\{1,\\ldots,P\\} \\] \\[\\phi_{v,p} \\stackrel{iid}{\\sim} C^+(0,1) \\quad v \\in \\{1,\\ldots,V_p\\} \\quad p \\in \\{1,\\ldots,P\\} \\] For the spike indicators, the prior is set as: \\[ \\xi_p \\stackrel{iid}{\\sim} Ber(\\nu) \\] \\[ \\nu \\sim Beta(a_\\nu, b_\\nu) \\] where a Beta prior is assigned to the Bernoulli prior probability to account for multiplicity correction. Inverse-Gamma priors are assigned to the error variances: \\[ \\tau^2_\\theta \\sim IG(a_{\\tau_\\theta}, b_{\\tau_\\theta}) \\] \\[ \\tau^2_\\beta \\sim IG(a_{\\tau_\\beta}, b_{\\tau_\\beta}) \\] Finally, for the other covariate coefficients: \\[ \\psi_{p,h}^a,\\psi^g_{p,h} \\stackrel{iid}{\\sim}N(0,1) \\quad p \\in \\{1,\\ldots,P\\}, \\quad h \\in \\{1,\\ldots,H\\} \\] References "],["posteriorComputation.html", "Chapter 3 Posterior Computation 3.1 Full Conditional for \\( {\\boldsymbol \\psi} ^a_{p,.}\\) 3.2 Full conditional for \\( {\\boldsymbol \\psi} ^g_{p,.}\\) 3.3 Full conditional for \\(\\xi_p\\), \\( {\\boldsymbol \\Theta} _{-p,p}\\) and \\( {\\boldsymbol \\beta} _p\\) 3.4 Full Conditional for \\(\\tau^2_\\theta\\) 3.5 Full Conditional for \\(\\tau^2_\\beta\\) 3.6 Full conditional \\(\\nu\\) 3.7 Full Conditional for the Network Coefficients Horseshoe Structure", " Chapter 3 Posterior Computation Bayesian estimation of the model is performed through Gibbs sampling which cycles through the following steps: For each \\(p\\in\\{1,...,P\\}\\) sample from \\( {\\boldsymbol \\psi} ^a_{p,.}\\), from the full conditional \\(p( {\\boldsymbol \\psi} ^a_{p,.}| {\\boldsymbol \\psi} ^a_{-p,.}, {\\boldsymbol \\Theta} ,\\tau_\\theta^2, {\\boldsymbol A} )\\). For each \\(p\\in\\{1,...,P\\}\\) sample from \\( {\\boldsymbol \\psi} ^g_{p,.}\\), from the full conditional \\(p( {\\boldsymbol \\psi} ^g_{p,.}| {\\boldsymbol \\beta} _p,\\tau^2_\\beta, {\\boldsymbol g} _p)\\). For each \\(p\\in\\{1,...,P\\}\\) sample jointly \\(\\xi_p\\), \\( {\\boldsymbol \\Theta} _{-p,p}\\) and \\( {\\boldsymbol \\beta} _p\\) from: \\[\\begin{align*} p(\\xi_p, {\\boldsymbol \\Theta} _{-p,p}, {\\boldsymbol \\beta} _p &amp;|\\nu, \\tau_\\theta^2, \\tau_\\beta^2, \\sigma_{\\theta}^2, \\eta_p^2, {\\boldsymbol \\Lambda} _{-p,p}, \\phi_p^2, {\\boldsymbol A} , {\\boldsymbol G} ) \\\\ &amp; = p( {\\boldsymbol \\Theta} _{-p,p}, {\\boldsymbol \\beta} _p | {\\boldsymbol \\xi} , \\tau_\\theta^2, \\tau_\\beta^2, \\sigma_{\\theta}^2, \\eta_p^2, {\\boldsymbol \\Lambda} _{-p,p}, \\phi_p^2, {\\boldsymbol A} , {\\boldsymbol G} ) \\\\ &amp; \\quad \\times p(\\xi_p|\\nu, {\\boldsymbol \\xi} _{-p}, \\tau_\\theta^2, \\tau_\\beta^2, \\sigma_{\\theta}^2, \\eta_p^2, {\\boldsymbol \\Lambda} _{-p,p}, \\phi_p^2, {\\boldsymbol A} , {\\boldsymbol G} ) \\end{align*}\\] Sample \\(\\tau^2_\\theta\\) from the full conditional \\(p(\\tau^2_\\theta| {\\boldsymbol \\xi} , {\\boldsymbol \\Theta} , \\sigma_{\\theta}^2, {\\boldsymbol \\Lambda} , {\\boldsymbol A} )\\) Sample \\(\\tau^2_\\beta\\) from the full conditional: \\(p(\\tau^2_\\beta| {\\boldsymbol \\xi} , {\\boldsymbol \\beta} _1,..., {\\boldsymbol \\beta} _P, \\eta_1^2, ...,\\eta_P^2, \\phi_1^2, ...,\\phi_P^2, {\\boldsymbol G} )\\) Sample \\(\\nu\\) from the full conditional \\(p(\\nu| {\\boldsymbol \\xi} )\\). Sample the horseshoe parameters using the latent variable approach as in (Makalic and Schmidt 2016). 3.1 Full Conditional for \\( {\\boldsymbol \\psi} ^a_{p,.}\\) \\[ {\\boldsymbol \\psi} ^a_{p,.}| {\\boldsymbol \\psi} ^a_{-p,.}, {\\boldsymbol \\Theta} ,\\sigma^2, {\\boldsymbol A} \\sim N \\left( \\hat{ {\\boldsymbol \\psi} }^a_{p,.}, \\tau_\\theta^2 ( {\\boldsymbol W} &#39; {\\boldsymbol W} )^{-1} \\right)\\] where \\[\\begin{align*} \\hat{ {\\boldsymbol \\psi} }^a_{p,.} &amp;= ( {\\boldsymbol W} &#39; {\\boldsymbol W} )^{-1} {\\boldsymbol W} &#39; {\\boldsymbol R} \\\\ {\\boldsymbol R} &amp;= [ ( {\\boldsymbol R} _1)_{-p,p}&#39;,...,( {\\boldsymbol R} _n)_{-p,p}&#39; ]&#39; \\\\ {\\boldsymbol W} &amp;= [ {\\boldsymbol W} _1&#39;,..., {\\boldsymbol W} _n&#39;]&#39; \\\\ ( {\\boldsymbol R} _i)_{-p,p} &amp;= ( {\\boldsymbol A} _i)_{-p,p} - {\\boldsymbol \\Theta} _{-p,p}y_i \\quad \\forall i \\in \\{1,...,n\\} \\\\ {\\boldsymbol W} _i &amp;= {\\boldsymbol \\psi} ^a_{-p,.} \\cdot ({\\boldsymbol 1}_P \\otimes {\\boldsymbol x} _i&#39;) \\quad \\forall i \\in \\{1,...,n\\}. \\\\ \\end{align*}\\] 3.2 Full conditional for \\( {\\boldsymbol \\psi} ^g_{p,.}\\) \\[ {\\boldsymbol \\psi} ^g_{p,.}| {\\boldsymbol \\beta} _p,\\sigma^2, {\\boldsymbol g} _p \\sim N \\left( \\hat{ {\\boldsymbol \\psi} }^g_{p,.}, \\tau_\\beta^2 ( {\\boldsymbol W} &#39; {\\boldsymbol W} )^{-1} \\right)\\] where \\[\\begin{align*} \\hat{ {\\boldsymbol \\psi} }^g_{p,.} &amp;= ( {\\boldsymbol W} &#39; {\\boldsymbol W} )^{-1} {\\boldsymbol W} &#39; {\\boldsymbol R} \\\\ {\\boldsymbol R} &amp;= [ ( {\\boldsymbol R} _1)_{-p,p}&#39;,...,( {\\boldsymbol R} _n)_{-p,p}&#39; ]&#39; \\\\ {\\boldsymbol W} &amp;= [ {\\boldsymbol W} _1&#39;,..., {\\boldsymbol W} _n&#39;]&#39; \\\\ ( {\\boldsymbol R} _i)_{-p,p} &amp;= {\\boldsymbol g} _{i,p} - {\\boldsymbol \\beta} _p y_i \\quad \\forall i \\in \\{1,...,n\\} \\\\ {\\boldsymbol W} _i &amp;= {\\boldsymbol 1}_{V_p} \\otimes {\\boldsymbol x} _i&#39; \\quad \\forall i \\in \\{1,...,n\\}. \\\\ \\end{align*}\\] 3.3 Full conditional for \\(\\xi_p\\), \\( {\\boldsymbol \\Theta} _{-p,p}\\) and \\( {\\boldsymbol \\beta} _p\\) To sample \\( {\\boldsymbol \\Theta} _{-p,p}, {\\boldsymbol \\beta} _p\\) set them to \\( {\\boldsymbol 0} \\) if \\(\\xi_p = 0\\), if \\(\\xi=1\\) then there are 2 cases. If \\(\\xi_{p&#39;} = 0 \\quad \\forall p&#39;\\) then set \\( {\\boldsymbol \\Theta} _{-p,p} = {\\boldsymbol 0} \\) and sample \\( {\\boldsymbol \\beta} _p\\) from: \\[ {\\boldsymbol \\beta} _p |\\tau_\\beta^2, \\eta_p^2, {\\boldsymbol \\phi} _p^2, {\\boldsymbol G} \\sim N(\\hat{b}, \\tau_\\beta^2 \\text{diag}(1/(S_{yy}^2 + 1 / {\\boldsymbol L} )) )\\] where \\[\\begin{align*} \\hat{b} &amp;= {\\boldsymbol S} _{xy} / (S_{yy}^2 + 1 / {\\boldsymbol L} ) \\\\ {\\boldsymbol S} _{xy} &amp;= {\\boldsymbol S} _{gy,p} \\\\ S_{yy}^2 &amp;= {\\boldsymbol y} &#39; {\\boldsymbol y} \\\\ {\\boldsymbol L} &amp;= \\eta_p^2 ( {\\boldsymbol \\phi} _p^2) \\\\ ( {\\boldsymbol S} _{gy,p})_{v} &amp;= \\sum_{i=1}^n R_{i,v,p}^g y_i \\\\ R_{i,v,p}^g &amp;= {\\boldsymbol g} _{i,(v,p)} - \\sum_{h=1}^H \\psi^g_{p,h} x_{i,h} \\\\ \\end{align*}\\] And sample \\(\\xi_p\\) from \\[ \\xi_p|\\nu, \\tau_\\beta^2, \\eta_p^2, {\\boldsymbol \\phi} _p^2, {\\boldsymbol G} \\sim Bernoulli(\\hat{v}_p)\\] where \\[\\begin{align*} \\hat{v}_p &amp;= o_p / (1 + o_p) \\\\ o_p &amp;= \\exp(\\sum_{i=1} c_i) \\frac{\\nu}{1-\\nu}\\\\ c_i &amp;= -\\frac{1}{2} \\left(\\log( {\\boldsymbol L} _i) + \\log\\left(S_{yy}^2 +\\frac{1}{ {\\boldsymbol L} _i}\\right) \\right) + \\hat{b}^2_i \\frac{S_{yy}^2 +\\frac{1}{ {\\boldsymbol L} _i}}{2\\tau_\\beta^2}\\\\ \\end{align*}\\] where \\( {\\boldsymbol L} _i\\) and \\(\\hat{b}\\) are as in the sampling of \\( {\\boldsymbol \\beta} _p\\). If \\(\\exists p&#39; \\ni \\xi_p \\neq0\\), then sample \\( {\\boldsymbol \\Theta} _{-p,p}\\) and \\( {\\boldsymbol \\beta} _p\\) from: \\[ {\\boldsymbol \\Theta} _{-p,p}, {\\boldsymbol \\beta} _p | {\\boldsymbol \\xi} _{-p}, \\tau_\\beta^2, \\tau_\\theta^2 \\sigma_{\\theta}^2, \\eta_p^2, {\\boldsymbol \\Lambda} _{-p,p}, \\phi_p^2, {\\boldsymbol A} , {\\boldsymbol G} \\sim N(\\hat{b}, \\text{diag}(\\tau/(S_{yy}^2 + 1 / {\\boldsymbol L} ) ) )\\] where \\[\\begin{align*} \\hat{b} &amp;= {\\boldsymbol S} _{xy} / (S_{yy}^2 + 1 / {\\boldsymbol L} ) \\\\ {\\boldsymbol S} _{xy} &amp;= [ ( {\\boldsymbol S} _{ay})_{p, -p}[ {\\boldsymbol \\xi} _{-p} = 1]&#39;, {\\boldsymbol S} _{gy,p}&#39;]&#39; \\\\ S_{yy}^2 &amp;= {\\boldsymbol y} &#39; {\\boldsymbol y} \\\\ \\tau &amp;= (\\tau_\\theta^2 {\\boldsymbol 1} _P[ {\\boldsymbol \\xi} _{-p} = 1]&#39;, \\tau_\\beta^2 {\\boldsymbol 1} _{V_p}&#39;)&#39; \\\\ {\\boldsymbol L} &amp;= [\\sigma^2_{\\theta} {\\boldsymbol \\Lambda} _{p, -p}[ {\\boldsymbol \\xi} _{-p} = 1]&#39;,\\eta_p^2 ( {\\boldsymbol \\phi} _p^2)&#39;]&#39; \\\\ ( {\\boldsymbol S} _{ay})_{p,p&#39;} &amp;= \\sum_{i=1}^n (R_i^a)_{p,p&#39;}y_i \\\\ ( {\\boldsymbol S} _{gy,p})_{v} &amp;= \\sum_{i=1}^n R_{i,v,p}^g y_i \\\\ (R_i^a)_{p,p&#39;} &amp;= ( {\\boldsymbol A} _i)_{p,p&#39;} - \\sum_{h=1}^H (\\psi^a)_{p,h} (\\psi^a)_{p&#39;,h} x_{i,h} \\\\ R_{i,v,p}^g &amp;= {\\boldsymbol g} _{i,(v,p)} - \\sum_{h=1}^H \\psi^g_{p,h} x_{i,h} \\\\ \\end{align*}\\] We sample \\(\\xi_p\\) from \\[ \\xi_p|\\nu, {\\boldsymbol \\xi} _{-p}, \\tau_\\theta^2, \\tau_\\beta^2, \\sigma_{\\theta}^2, \\eta_p^2, {\\boldsymbol \\Lambda} _{-p,p}, \\phi_p^2, {\\boldsymbol A} , {\\boldsymbol G} \\sim Bernoulli(\\hat{v}_p)\\] where \\[\\begin{align*} \\hat{v}_p &amp;= o_p / (1 + o_p) \\\\ o_p &amp;= \\exp(\\sum_{i=1} c_i) \\frac{\\nu}{1-\\nu}\\\\ c_i &amp;= -\\frac{1}{2} \\left(\\log( {\\boldsymbol L} _i) + \\log\\left(S_{yy}^2 +\\frac{1}{ {\\boldsymbol L} _i}\\right) \\right) + \\hat{b}_i \\frac{S_{yy}^2 +\\frac{1}{ {\\boldsymbol L} _i}}{2\\tau^2}\\\\ \\end{align*}\\] where \\( {\\boldsymbol L} _i\\) and \\(\\hat{b}\\) and \\(\\tau\\) are as in the sampling of \\( {\\boldsymbol \\Theta} _{-p,p}, {\\boldsymbol \\beta} _p\\). 3.4 Full Conditional for \\(\\tau^2_\\theta\\) \\[ \\tau^2_\\theta| {\\boldsymbol \\xi} , {\\boldsymbol \\Theta} , \\sigma_{\\theta}^2, {\\boldsymbol \\Lambda} , {\\boldsymbol A} \\sim IG(\\hat{a}_{\\tau_\\theta}, \\hat{b}_{\\tau\\theta}) \\] where \\[\\begin{align*} \\hat{b}_{\\tau_\\theta} &amp; = b_{\\tau_\\theta} + \\frac{\\sum_{i=1}^n \\sum_{p&lt;p&#39;} (R^a_i)_{p,p&#39;}}{2} \\\\ &amp;+ \\frac{\\sum_{p&lt;p&#39;} {\\boldsymbol \\Theta} _{ {\\boldsymbol \\xi} = 1, {\\boldsymbol \\xi} = 1}^2 / {\\boldsymbol \\Lambda} _{ {\\boldsymbol \\xi} = 1, {\\boldsymbol \\xi} = 1} / \\sigma^2_{\\theta}}{2} \\\\ \\hat{a}_{\\tau_\\theta} &amp; = a_{\\tau_\\theta} + \\frac{\\frac{np(p-1)}{2} + \\frac{nq(q-1)}{2}}{2} \\\\ R_i^a &amp;= \\left( {\\boldsymbol A} _i - \\sum_{h=1}^{H} {\\boldsymbol \\psi} _{h}^a ( {\\boldsymbol \\psi} _{h}^{a})&#39; x_{i,h} - {\\boldsymbol \\Theta} \\: y_i \\right)^2 \\\\ \\end{align*}\\] 3.5 Full Conditional for \\(\\tau^2_\\beta\\) \\[ \\tau^2_\\beta| {\\boldsymbol \\xi} , {\\boldsymbol \\beta} _1,..., {\\boldsymbol \\beta} _P, \\eta_1^2, ...,\\eta_P^2, \\phi_1^2, ...,\\phi_P^2, {\\boldsymbol G} \\sim IG(\\hat{a}_{\\tau_\\beta}, \\hat{b}_{\\tau_\\beta}) \\] where \\[\\begin{align*} \\hat{b}_{\\tau_\\beta} &amp;= b_{\\tau_\\beta} + \\frac{\\sum_{i=1}^n \\sum_{p=1}^P R^g_{i,p} + \\sum_{p=1}^P \\xi_p {\\boldsymbol \\beta} _p^2 / (\\eta_p \\phi^2_p)}{2} \\\\ \\hat{a}_{\\tau_\\beta} &amp;= a_{\\tau_\\beta} + \\frac{n \\sum_{p=1}^P V_p + n \\sum_{p=1}^P V_p \\xi_p}{2} \\\\ R_{i,p}^g &amp;= \\left( {\\boldsymbol g} _{i,p} - \\sum_{h=1}^{H} {\\boldsymbol 1}_{V_p}\\psi_{p,h}^g x_{i,h} - {\\boldsymbol \\beta} _{p}\\: y_i \\right)^2 \\end{align*}\\] 3.6 Full conditional \\(\\nu\\) \\[ \\nu|\\xi_1,\\ldots,\\xi_p \\sim Beta \\left(a_\\nu + \\sum_{p=1}^P \\xi_p, b_{\\nu}+P-\\sum_{p=1}^P \\xi_p \\right) \\] 3.7 Full Conditional for the Network Coefficients Horseshoe Structure Following (Makalic and Schmidt 2016) auxiliary variables are used for the priors of the \\(\\lambda_{p,p&#39;}\\)’s and \\(\\sigma^2_\\theta\\), to have that: \\[ \\lambda_{p,p&#39;} \\sim IG(1/2, 1/\\kappa_{\\lambda_{p,p&#39;}}) \\] \\[ \\sigma^2_\\theta \\sim IG(1/2, 1/\\kappa_{\\sigma^2_\\theta}) \\] \\[ \\phi_{v,p} \\sim IG(1/2, 1/\\kappa_{\\phi_{v,p}}) \\] \\[ \\eta_p \\sim IG(1/2, 1/\\kappa_{\\eta_p}) \\] \\[ \\kappa_{\\lambda_{p,p}}, \\kappa_{\\sigma_\\theta}, \\kappa_{\\phi_{v,p}}, \\kappa_{\\eta_p} \\stackrel{iid}{\\sim} IG(1/2, 1) \\quad v \\in \\{1,\\ldots,V_p\\} \\quad p \\in \\{1,\\ldots,P\\} \\] Then the full conditionals for the Network Horseshoe parameters (including auxiliary variables) are given by \\[ \\lambda^2_{p,p&#39;}| \\sigma^2_\\theta, \\tau^2_\\theta, \\theta_{p,p&#39;}, \\xi_p, \\xi_{p&#39;} \\stackrel{ind.}{\\sim} IG \\left(1, \\frac{1}{\\kappa_{\\lambda_{p,p&#39;}}}+I_{\\xi_p=1}I_{\\xi_{p&#39;}=1}\\frac{\\theta^2_{p,p&#39;}}{2 \\sigma^2_\\theta \\tau^2_\\theta}\\right) \\quad p&lt;p&#39;\\] \\[ \\sigma^2_\\theta| {\\boldsymbol \\Lambda} , \\tau^2_\\theta, {\\boldsymbol \\Theta} , \\xi_p \\stackrel{ind.}{\\sim} IG \\left(1, \\frac{1}{\\kappa_{\\sigma^2_\\theta}} + \\frac{1}{2 \\tau^2_\\theta} \\sum_{\\substack{p&lt;p&#39; \\\\ \\xi_p = 1 \\\\ \\xi_{p&#39;}=1}}\\frac{\\theta_{p,p&#39;}^2}{\\tau^2_{\\theta}\\lambda^2_{p,p&#39;}} \\right) \\] \\[ \\kappa_{\\lambda_{p,p&#39;}}|\\lambda^2_{p,p&#39;} \\sim IG \\left(1, 1 + \\frac{1}{\\lambda^2_{p,p&#39;}}\\right) \\] \\[ \\kappa_{\\sigma_\\theta}|\\sigma^2_\\theta \\sim IG \\left(1, 1 + \\frac{1}{\\sigma^2_\\theta}\\right) \\] Then the full conditionals for the Voxel Horseshoe parameters (including auxiliary variables) are given by \\[ \\phi^2_{v,p}| \\eta^2_p, \\tau^2_\\beta, \\phi_{v,p}, \\xi_p \\stackrel{ind.}{\\sim} IG \\left(1, \\frac{1}{\\kappa_{\\phi_{v,p}}}+I_{\\xi_p=1}\\frac{ \\beta^2_{v,p}}{2 \\eta^2_p \\tau^2_\\beta}\\right) \\quad v \\in \\{1,\\ldots,V_p\\} \\quad p \\in \\{1,\\ldots,P\\} \\] \\[ \\eta^2_p| {\\boldsymbol \\Phi} , \\tau^2_\\beta, {\\boldsymbol B} , \\xi_p \\stackrel{ind.}{\\sim} IG \\left(1, \\frac{1}{\\kappa_{\\eta_P}} + \\frac{I_{\\xi_p = 1}}{2 \\tau^2_\\beta} \\sum_{\\substack{v=1}}^{V_p}\\frac{\\beta_{v,p}^2}{\\tau^2_{\\theta}\\lambda^2_{p,p&#39;}} \\right) \\quad p \\in \\{1,\\ldots,P\\} \\] \\[ \\kappa_{\\phi_{v,p}}|\\phi^2_{v,p} \\sim IG \\left(1, 1 + \\frac{1}{\\phi^2_{v,p}}\\right) \\] \\[ \\kappa_{\\eta_p}|\\eta^2_p \\sim IG \\left(1, 1 + \\frac{1}{\\eta^2_p}\\right) \\quad p \\in \\{1,\\ldots,P\\} \\] References "],["variableRelations.html", "Chapter 4 Variable Relations", " Chapter 4 Variable Relations "],["references.html", "References", " References "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
